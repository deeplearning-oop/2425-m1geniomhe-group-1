import numpy as np
from functools import wraps
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def viz_generated_data(func):
    """
    Visualizes data generated by a given function.
    
    Parameters:
        func (callable): Function that generates data (returns X, y).
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        X, y = func(*args, **kwargs)
        
        plt.figure(figsize=(6, 6))
        plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Class 0')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1')
        plt.xticks([])
        plt.yticks([])
        plt.legend()
        plt.title("Generated Data Visualization")
        plt.show()
        
        return X, y
    
    return wrapper

def generate_circular_data(n_samples=500, noise=0.1):
    n_samples_out = n_samples // 2
    n_samples_in = n_samples - n_samples_out
    r_out = 1.0
    r_in = 0.5
    theta_out = 2 * np.pi * np.random.rand(n_samples_out)
    x_out = r_out * np.cos(theta_out) + noise * np.random.randn(n_samples_out)
    y_out = r_out * np.sin(theta_out) + noise * np.random.randn(n_samples_out)
    theta_in = 2 * np.pi * np.random.rand(n_samples_in)
    x_in = r_in * np.cos(theta_in) + noise * np.random.randn(n_samples_in)
    y_in = r_in * np.sin(theta_in) + noise * np.random.randn(n_samples_in)
    X = np.vstack((np.column_stack((x_out, y_out)), np.column_stack((x_in, y_in))))
    y = np.hstack((np.ones(n_samples_out), np.zeros(n_samples_in)))
    return X, y

def generate_spiral_data(n_samples=500, noise=0.1):
    n = np.sqrt(np.random.rand(n_samples // 2)) * 780 * (2 * np.pi) / 360
    d1x = -np.cos(n) * n + noise * np.random.randn(n_samples // 2)
    d1y = np.sin(n) * n + noise * np.random.randn(n_samples // 2)
    X1 = np.column_stack((d1x, d1y))
    y1 = np.zeros(n_samples // 2)
    d2x = np.cos(n) * n + noise * np.random.randn(n_samples // 2)
    d2y = -np.sin(n) * n + noise * np.random.randn(n_samples // 2)
    X2 = np.column_stack((d2x, d2y))
    y2 = np.ones(n_samples // 2)
    X = np.vstack((X1, X2))
    y = np.hstack((y1, y2))
    return X, y

def generate_checkerboard_data(n_samples=500, grid_size=4, noise=0.1):
    x = np.random.rand(n_samples) * grid_size
    y = np.random.rand(n_samples) * grid_size
    labels = ((np.floor(x) + np.floor(y)) % 2).astype(int)
    X = np.column_stack((x, y)) + noise * np.random.randn(n_samples, 2)
    return X, labels

def generate_linearly_separable_data(n_samples=500, noise=0.1):
    X = np.random.randn(n_samples, 2)
    y = (X[:, 1] > X[:, 0]).astype(int)
    X += noise * np.random.randn(n_samples, 2)
    return X, y

@viz_generated_data
def generate_data(choice=1, n_samples=500, noise=0.1, grid_size=4):
    """
    Generates data based on user input or random selection.
    Parameters:
        choice (str or int): One of ['circular', 'spiral', 'checkerboard', 'linear'] or an integer to select by %4.
        n_samples (int): Number of samples to generate.
        noise (float): Noise level to add to the data.
        grid_size (int): Grid size for checkerboard data.
    Returns:
        X (array): Features.
        y (array): Labels.
    """
    if isinstance(choice, int):
        choice = ['circular', 'spiral', 'checkerboard', 'linear'][choice % 4]
    
    if choice == 'circular':
        print('  -- loading circular data')
        return generate_circular_data(n_samples, noise)
    elif choice == 'spiral':
        print('  -- loading spiral data')
        return generate_spiral_data(n_samples, noise)
    elif choice == 'checkerboard':
        print('  -- loading checkerboard data')
        return generate_checkerboard_data(n_samples, grid_size, noise)
    else:
        print('  -- loading linear data')
        return generate_linearly_separable_data(n_samples, noise)


from tensor import Tensor
from dataset import TensorDataset
from module import Module
from linear import Linear
from activation import ReLU, Softmax
from loss import CrossEntropyLoss,MSE
from optimizer import SGD
from transforms import Standardize, ToTensor, Compose

X, y = generate_data(choice='linear', n_samples=500, noise=0.1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -- testing if it workd on ndarray
# training_data=Tensor(X_train)
# training_labels=Tensor(y_train)
# test_data=Tensor(X_test)
# test_labels=Tensor(y_test)

# transformation=Compose([ToTensor(), Standardize()])
transformation=Compose([])

train_dataset = TensorDataset(X_train, y_train, transform=transformation)
test_dataset = TensorDataset(X_test, y_test, transform=transformation)

print(f'train dataset of length {len(train_dataset)} and shape {train_dataset[0][0].shape}; labels shape {train_dataset[0][1].shape}')
print(f'test dataset of length {len(test_dataset)} and shape {test_dataset[0][0].shape}; labels shape {test_dataset[0][1].shape}')

print()

from dataloader import DataLoader
train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader=DataLoader(test_dataset, batch_size=32, shuffle=True)

for batch,(i,j) in enumerate(train_loader):
    print(f'batch {batch} data shape {i.shape}; labels shape {j.shape}')

class SimpleNN(Module):
    def __init__(self):
        super().__init__()
        self.fc1=Linear(2, 3)
        self.fc2=Linear(3, 1)
        self.soft=Softmax()

    def forward(self, x):
        x=self.fc1(x)
        x=self.soft(x)
        x=self.fc2(x)
        return x
    
model = SimpleNN()
optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)
loss_fn = CrossEntropyLoss()

# -- training
for epoch in range(10):
    for batch_no,(x, y) in enumerate(train_loader):
        x.flatten_batch() #or just transpose, can do x.T now
        print(f'x shape: {x.shape}, y shape: {y.shape}')
        optimizer.zero_grad()
        y_hat = model(x)
        loss = loss_fn(y, y_hat)
        loss.backward()
        optimizer.step()
        

    print(f'iteration: {epoch}')    
    print(f'Loss: {loss.data}') 
    predictions = np.argmax(y_hat.data, axis=0)
    accuracy = np.sum(predictions == y.data) / y.data.size
    print(predictions, y.data)
    print(f'Accuracy: {accuracy * 100:.2f}%')
    print('------------------')