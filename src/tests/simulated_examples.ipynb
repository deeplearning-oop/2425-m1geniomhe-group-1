{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys,os\n",
    "os.chdir('..') #change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_simulation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly seperable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(choice='linear', n_samples=500, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor\n",
    "from dataset import TensorDataset\n",
    "from module import Module\n",
    "from linear import Linear\n",
    "from activation import ReLU\n",
    "from loss import CrossEntropyLoss,MSE\n",
    "from optimizer import SGD\n",
    "from transforms import Standardize, ToTensor, Compose\n",
    "\n",
    "# -- testing if it workd on ndarray\n",
    "# training_data=Tensor(X_train)\n",
    "# training_labels=Tensor(y_train)\n",
    "# test_data=Tensor(X_test)\n",
    "# test_labels=Tensor(y_test)\n",
    "\n",
    "transformation=Compose([ToTensor(), Standardize()])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train, transform=transformation)\n",
    "test_dataset = TensorDataset(X_test, y_test, transform=transformation)\n",
    "\n",
    "print(f'train dataset of length {len(train_dataset)} and shape {train_dataset[0][0].shape}; labels shape {train_dataset[0][1].shape}')\n",
    "print(f'test dataset of length {len(test_dataset)} and shape {test_dataset[0][0].shape}; labels shape {test_dataset[0][1].shape}')\n",
    "\n",
    "print()\n",
    "\n",
    "from dataloader import DataLoader\n",
    "train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch,(x,y) in enumerate(train_loader):\n",
    "    print(f'batch {batch} data shape {x.shape}; labels shape {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.arange(y.data.size),y.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1=Linear(2, 5)\n",
    "        self.fc2=Linear(5, 1)\n",
    "        self.relu=ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleNN()\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "loss_fn = MSE()\n",
    "\n",
    "# -- training\n",
    "for epoch in range(10):\n",
    "    for batch_no,(x, y) in enumerate(train_loader):\n",
    "        x.flatten_batch()\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'iteration: {epoch}')    \n",
    "    print(f'Loss: {loss.data}') \n",
    "    predictions = np.argmax(y_hat.data, axis=0)\n",
    "    accuracy = np.sum(predictions == y.data) / y.data.size\n",
    "    print(predictions, y.data)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print('------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
