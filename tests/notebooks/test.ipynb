{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, is_leaf=False):    \n",
    "        self.data = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "        self.requires_grad = requires_grad\n",
    "        self.is_leaf = is_leaf\n",
    "        self.grad = None\n",
    "        self.grad_fn = None\n",
    "        self.grad_fn_name = None\n",
    "        self.parents = set()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''important for dataset and dataloaders'''\n",
    "        return self.data[idx]\n",
    "\n",
    "    # ----- to flatten images --------\n",
    "    def view(self,*args):\n",
    "        '''same as torch's functionality, it collapses all dimensions into 1\n",
    "        \n",
    "        <!> to be tested sperately on example tensors\n",
    "        '''\n",
    "        nd_array=self.data\n",
    "        reshaped= nd_array.reshape(args)\n",
    "        t= self\n",
    "        t.data=reshaped\n",
    "        return t\n",
    "    \n",
    "    def flatten_batch(self):\n",
    "        '''\n",
    "        given that a tenosr is a batch of length batch_size, it'll flatten the dimensions while conserving the batch dimension (and transpsoing to match pytorch's behavior)\n",
    "    \n",
    "        e.g., it will take (batch_size,1,28,28) and return (784, batch_size) \n",
    "\n",
    "        <!> used for testing while batch training images\n",
    "        '''\n",
    "        flattened = np.array([img.flatten() for img in self.data])  # Shape: (32, 784)\n",
    "        transposed = flattened.T  # Shape: (784, 32)\n",
    "\n",
    "        self.data = transposed\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"AddBackward\"\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        \n",
    "        result = Tensor(-self.data, requires_grad=self.requires_grad)\n",
    "        result.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = -grad\n",
    "                else:\n",
    "                    self.grad -= grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"NegBackward\"\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.data - other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "        \n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad\n",
    "                else:\n",
    "                    other.grad -= grad\n",
    "        \n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"SubBackward\"\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Handle the case when 'other' is a scalar (e.g., a float or int)\n",
    "        if isinstance(other, (int, float)) or isinstance(self, (int, float)):\n",
    "            # Scalar multiplication: Multiply the scalar with the data and return a new Tensor\n",
    "            out = Tensor(self.data * other, requires_grad=self.requires_grad)\n",
    "            out.parents = {self}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other  # Gradient w.r.t. the scalar\n",
    "                    else:\n",
    "                        self.grad += grad * other  # Accumulate gradient w.r.t. the scalar\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ScalarMulBackward\"\n",
    "            return out\n",
    "        \n",
    "        # Handle the case when 'other' is a Tensor\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.data * other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "            out.parents = {self, other}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other.data  # Gradient w.r.t. the other Tensor\n",
    "                    else:\n",
    "                        self.grad += grad * other.data  # Accumulate gradient w.r.t. the other Tensor\n",
    "                if other.requires_grad:\n",
    "                    if other.grad is None:\n",
    "                        other.grad = grad * self.data  # Gradient w.r.t. self Tensor\n",
    "                    else:\n",
    "                        other.grad += grad * self.data  # Accumulate gradient w.r.t. self Tensor\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"TensorMulBackward\"\n",
    "            return out\n",
    "\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data / other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / other.data\n",
    "                else:\n",
    "                    self.grad += grad / other.data\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad * self.data / (other.data ** 2)\n",
    "                else:\n",
    "                    other.grad -= grad * self.data / (other.data ** 2)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"DivBackward\"\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        out = Tensor(self.data.mean(), requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / self.data.size\n",
    "                else:\n",
    "                    self.grad += grad / self.data.size\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MeanBackward\"\n",
    "        return out\n",
    "\n",
    "    def sum(self):\n",
    "        out = Tensor(self.data.sum(), requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * np.ones_like(self.data)\n",
    "                else:\n",
    "                    self.grad += grad * np.ones_like(self.data)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"SumBackward\"\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # Apply ReLU: max(0, x)\n",
    "        out_data = np.maximum(self.data, 0)\n",
    "\n",
    "        # Create a new tensor for the result\n",
    "        out = Tensor(out_data, requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        if self.requires_grad:\n",
    "            # Define the backward pass for ReLU\n",
    "            def _backward(grad):\n",
    "                # The derivative of ReLU is 1 for positive values, 0 for negative\n",
    "                relu_grad = (self.data > 0).astype(float)  # Create mask for positive values\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * relu_grad\n",
    "                else:\n",
    "                    self.grad += grad * relu_grad\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ReLUBackward\"\n",
    "        return out\n",
    "\n",
    "    def softmax(self):\n",
    "        # Apply softmax to logits for numerical stability\n",
    "        max_logits = np.max(self.data, axis=0, keepdims=True)  # Shape (1, N)\n",
    "        exps = np.exp(self.data - max_logits)\n",
    "        sum_exps = np.sum(exps, axis=0, keepdims=True)\n",
    "        result = exps / sum_exps\n",
    "        # result = np.exp(self.data) / sum(np.exp(self.data))\n",
    "        \n",
    "        out = Tensor(result, requires_grad=self.requires_grad)  # Output tensor\n",
    "        out.parents = {self}  # Store parent tensors\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _backward(grad):\n",
    "                \n",
    "                # Compute softmax of the input\n",
    "                # softmax = exps / sum_exps  # Compute softmax\n",
    "                # Gradient of log-softmax\n",
    "                # grad_input = grad - np.sum(grad, axis=-1, keepdims=True) * softmax  # Backpropagate\n",
    "                grad_input = result * (grad - np.sum(grad * result, axis=0, keepdims=True))\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad_input  # Initialize grad if it's None\n",
    "                else:\n",
    "                    self.grad += grad_input  # Accumulate gradients if grad already exists\n",
    "\n",
    "                return grad  # Return gradient input for the next layer\n",
    "\n",
    "            out.grad_fn = _backward  # Store the backward function\n",
    "            out.grad_fn_name = \"LogSoftmaxBackward\"\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # def log(self):\n",
    "    #     # Handle log of zero by adding a small epsilon\n",
    "    #     out = Tensor(np.log(self.data + 1e-9), requires_grad=self.requires_grad)\n",
    "    #     out._prev = {self}\n",
    "\n",
    "    #     def _backward(grad):\n",
    "    #         if self.requires_grad:\n",
    "    #             if self.grad is None:\n",
    "    #                 self.grad = grad / (self.data + 1e-9)\n",
    "    #             else:\n",
    "    #                 self.grad += grad / (self.data + 1e-9)\n",
    "\n",
    "    #     out.grad_fn = _backward\n",
    "    #     out.grad_fn_name = \"LogBackward\"\n",
    "    #     return out\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        out = Tensor(self.data ** power, requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * power * (self.data ** (power - 1))\n",
    "                else:\n",
    "                    self.grad += grad * power * (self.data ** (power - 1))\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"PowBackward\"\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data @ other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad @ other.data.T\n",
    "                else:\n",
    "                    self.grad += grad @ other.data.T\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = self.data.T @ grad\n",
    "                else:\n",
    "                    other.grad += self.data.T @ grad\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MatMulBackward\"\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        grad_fn_str = f\", grad_fn=<{self.grad_fn_name}>\" if self.grad_fn else \"\"\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad}{grad_fn_str})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \n",
    "        # Start the backward pass if this tensor requires gradients\n",
    "        if not self.requires_grad:\n",
    "            raise ValueError(\"This tensor does not require gradients.\")\n",
    "        \n",
    "        # Initialize the gradient for the tensor if not already set\n",
    "        if self.grad is None:\n",
    "            self.grad = np.ones_like(self.data)  # Start with gradient of 1 for scalar output\n",
    "            # self.grad = Tensor(self.grad)  # Convert to a tensor\n",
    "        \n",
    "        # A stack of tensors to backpropagate through\n",
    "        to_process = [self]\n",
    "        # Process the tensors in reverse order (topological order)\n",
    "        while to_process:\n",
    "            tensor = to_process.pop()\n",
    "            if tensor.is_leaf and tensor.data.shape != tensor.grad.shape:\n",
    "                tensor.grad = np.sum(tensor.grad,axis=1).reshape(-1,1)\n",
    "\n",
    "            # If this tensor has a backward function, call it\n",
    "            if tensor.grad_fn is not None:\n",
    "                # print(f\"Backpropagating through {tensor.grad_fn_name}\")\n",
    "                # Pass the gradient to the parent tensors\n",
    "                tensor.grad_fn(tensor.grad)\n",
    "                # print(tensor.grad)\n",
    "                # Add the parents of this tensor to the stack for backpropagation\n",
    "                to_process.extend(tensor.parents)\n",
    "                \n",
    "    def detach(self):\n",
    "        # Create a new tensor that shares the same data but has no gradient tracking\n",
    "        detached_tensor = Tensor(self.data, requires_grad=False)\n",
    "        detached_tensor.grad = self.grad  # Retain the gradient (but no computation graph)\n",
    "        detached_tensor.parents = set()  # Detach from the computation graph\n",
    "        detached_tensor._grad_fn = None  # Remove the function responsible for backward\n",
    "        detached_tensor._grad_fn_name = None\n",
    "        return detached_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    '''----------------- Data Types -----------------\n",
    "    ---------------------------------------------------\n",
    "    this Enum class is used to define the data types of the tensors,\n",
    "    it allows restricting the data types to the ones defined in the class\n",
    "    as well as easily converting the data types to numpy data types\n",
    "\n",
    "    Mechanism of action relies on:  \n",
    "        * access of dtype calue from a string representation of the data type\n",
    "        * __call__ method that allows conversion of data to the specified data type through numpy\n",
    "    \n",
    "        p.s. uint8 is used for images as pixel values are in the range [0,255] which fits exactly to 8 bits (2^8=256)  \n",
    "            => uint8 is the first unit of conversion from image to tensor and a memory saver\n",
    "            \n",
    "    ____________________________________________________________________________________________________________________________\n",
    "\n",
    "    '''\n",
    "\n",
    "    int32='int32'\n",
    "    int64='int64'\n",
    "    float32='float32'\n",
    "    float64='float64'\n",
    "    uint8='uint8'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        >>> DataType.float32([1,2,3])\n",
    "        array([1., 2., 3.], dtype=float32)\n",
    "        >>> dtype=DataType.int32  \n",
    "        >>> dtype(1.7)\n",
    "        array(1, dtype=int32)\n",
    "        '''\n",
    "        return np.array(x, dtype=self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(([1,2],[1,2])).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1.0,2,3], requires_grad=True)\n",
    "# a.requires_grad=True\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dtype=DataType('uint8')\n",
    "a=dtype(1.7)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "---------------------------------\n",
    "tensor module\n",
    "---------------------------------\n",
    "This module contains the implementation of the Tensor class, which is the core data structure used in the deep learning framework  \n",
    "The Tensor class is designed to mimic the behavior of PyTorch's tensor class, allowing for easy manipulation of data and automatic gradient computation through backpropagation (computation graph)\n",
    "\n",
    "The module also relies on an Enum class implementation of DataType, to allow to have a dtype attribute in Tensor of type Datatype\n",
    "which consists of:  \n",
    "    * int32\n",
    "    * int64  \n",
    "    * float32  \n",
    "    * float64  \n",
    "    * uint8\n",
    "p.s. these types will be aliased so tehy can be accessed by tehir names from all over the library,\n",
    "    and they will be used to convert data to the specified data type through numpy\n",
    "\n",
    "```python\n",
    ">>> import ouur_library as torch_wannabe\n",
    ">>> x=torch_wannabe.Tensor([1,2,3],dtype=torch_wannabe.float32)\n",
    "# design exaclty identical to pytorch's ui\n",
    "```\n",
    "\n",
    "More details will be explained in class implementations\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    '''----------------- Data Types -----------------\n",
    "    ---------------------------------------------------\n",
    "    this Enum class is used to define the data types of the tensors,\n",
    "    it allows restricting the data types to the ones defined in the class\n",
    "    as well as easily converting the data types to numpy data types\n",
    "\n",
    "    Mechanism of action relies on:  \n",
    "        * access of dtype calue from a string representation of the data type\n",
    "        * __call__ method that allows conversion of data to the specified data type through numpy\n",
    "    \n",
    "        p.s. uint8 is used for images as pixel values are in the range [0,255] which fits exactly to 8 bits (2^8=256)  \n",
    "            => uint8 is the first unit of conversion from image to tensor and a memory saver\n",
    "            \n",
    "    ____________________________________________________________________________________________________________________________\n",
    "\n",
    "    '''\n",
    "\n",
    "    int32='int32'\n",
    "    int64='int64'\n",
    "    float32='float32'\n",
    "    float64='float64'\n",
    "    uint8='uint8'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        >>> DataType.float32([1,2,3])\n",
    "        array([1., 2., 3.], dtype=float32)\n",
    "        >>> dtype=DataType.int32  \n",
    "        >>> dtype(1.7)\n",
    "        array(1, dtype=int32)\n",
    "        '''\n",
    "        return np.array(x, dtype=self.value)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.value==other\n",
    "\n",
    "# ------- aliasing to make them easily accessible --------\n",
    "int32=DataType.int32\n",
    "int64=DataType.int64\n",
    "float32=DataType.float32\n",
    "float64=DataType.float64\n",
    "uint8=DataType.uint8\n",
    "\n",
    "def validate_boolean(value, default=False):\n",
    "    '''takes a value makes sure its boolean, if its not sets it to default'''\n",
    "    if not isinstance(value, bool):\n",
    "        print(f'<!> invalid boolean, setting to {default}')\n",
    "        value=default\n",
    "    return value\n",
    "\n",
    "def validate_data_type(value, default=float32):\n",
    "    '''takes a value makes sure its a DataType, if its not sets it to default'''\n",
    "    datatypes=list(DataType.__members__.values())\n",
    "    datatypes_str=[str(i) for i in datatypes]\n",
    "    # print(datatypes)\n",
    "    if isinstance(value, str):\n",
    "        if value not in datatypes_str:\n",
    "            print(f'<!> invalid data type, setting to {default}')\n",
    "            value=default\n",
    "        else:\n",
    "            value=DataType[value]\n",
    "    if value not in datatypes:\n",
    "        print(f'<!> invalid data type, setting to {default}')\n",
    "        value=default\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def validate_non_int(value):\n",
    "    '''takes a value makes sure its not an int, return boolean\n",
    "            RuntimeError: Only Tensors of floating point and complex dtype can require gradients\n",
    "'''\n",
    "    if value==int32 or value==int64:\n",
    "        raise RuntimeError('Only Tensors of floating point and complex dtype can require gradients')\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, is_leaf=False, dtype=float32):    \n",
    "        ''' Tesnor constructor:\n",
    "        \n",
    "        * respecting encapsulation (private attributes + getters and setters)  \n",
    "        * handling data type conversion through firstly assigning dtype and handling the rest in __setattr__  \n",
    "        * default values for requires_grad and is_leaf are set\n",
    "        \n",
    "        Note: default dtype will be float32 (unlike torch which is an int)  \n",
    "            and that's to avoid possibel errors if requires_grad is set to True\n",
    "\n",
    "        e.g. on error that should be given when type is int and requires_grad is True:  \n",
    "\n",
    "        ```python\n",
    "        >>> a=torch.tensor([1,2,3], requires_grad=True)\n",
    "        RuntimeError: Only Tensors of floating point and complex dtype can require gradients\n",
    "        ```\n",
    "        '''   \n",
    "        self.__dtype = validate_data_type(dtype)     \n",
    "        self.__data = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "        self.__requires_grad = requires_grad\n",
    "        self.__is_leaf = is_leaf\n",
    "        self.grad = None\n",
    "        self.grad_fn = None\n",
    "        self.grad_fn_name = None\n",
    "        self.parents = set()\n",
    "\n",
    "    # -- getters and setters\n",
    "    # -- not initialized but derived:\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.__data.shape\n",
    "    def ndim(self):\n",
    "        return self.__data.ndim\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        '''\n",
    "        # direct access from data, \n",
    "        # since this getter can only be called after instanciating \n",
    "        # we will surely have data assigned\n",
    "        # ensures dynamic real-time access to data type\n",
    "        # most importantly: returns a DataType object\n",
    "        '''\n",
    "        # print('testing to see if it works')\n",
    "        return DataType[(self.__data.dtype)]\n",
    "    @dtype.setter\n",
    "    def dtype(self, value):\n",
    "        # data conversion is handlede here bcs when instanciating will go to setattr and data would nto be defined yet, so better do casting here\n",
    "        self.__dtype = value\n",
    "        self.__data=value(self.__data)\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.__data\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.__data = value  \n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        return self.__requires_grad\n",
    "    @requires_grad.setter\n",
    "    def requires_grad(self, value):\n",
    "        self.__requires_grad = value\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.__is_leaf\n",
    "    @is_leaf.setter\n",
    "    def is_leaf(self, value):\n",
    "        self.__is_leaf = value\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        # if name=='_Tensor__dtype':\n",
    "        #     # -- add a validate type function here that checks its within DataType and converts it if given a string\n",
    "            \n",
    "        #     value=validate_data_type(value)\n",
    "           \n",
    "\n",
    "        if name=='_Tensor__requires_grad':\n",
    "            value=validate_boolean(value, False)\n",
    "            if value==True:\n",
    "                # -- can not set it true if data type is int\n",
    "                validate_non_int(self.__dtype)\n",
    "                \n",
    "\n",
    "        super().__setattr__(name, value)\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''important for dataset and dataloaders'''\n",
    "        return self.__data[idx]\n",
    "\n",
    "    # ----- to flatten images --------\n",
    "    def view(self,*args):\n",
    "        '''same as torch's functionality, it collapses all dimensions into 1\n",
    "        \n",
    "        <!> to be tested sperately on example tensors\n",
    "        '''\n",
    "        nd_array=self.__data\n",
    "        reshaped= nd_array.reshape(args)\n",
    "        t= self\n",
    "        t.data=reshaped\n",
    "        return t\n",
    "    \n",
    "    def flatten_batch(self):\n",
    "        '''\n",
    "        given that a tenosr is a batch of length batch_size, it'll flatten the dimensions while conserving the batch dimension (and transpsoing to match pytorch's behavior)\n",
    "    \n",
    "        e.g., it will take (batch_size,1,28,28) and return (784, batch_size) \n",
    "\n",
    "        <!> used for testing while batch training images\n",
    "        '''\n",
    "        flattened = np.array([img.flatten() for img in self.__data])  # Shape: (32, 784)\n",
    "        transposed = flattened.T  # Shape: (784, 32)\n",
    "\n",
    "        self.__data = transposed\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.__data.shape\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.__data + other.data, requires_grad=self.__requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"AddBackward\"\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        \n",
    "        result = Tensor(-self.__data, requires_grad=self.__requires_grad)\n",
    "        result.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = -grad\n",
    "                else:\n",
    "                    self.grad -= grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"NegBackward\"\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.__data - other.data, requires_grad=self.__requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "        \n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad\n",
    "                else:\n",
    "                    other.grad -= grad\n",
    "        \n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"SubBackward\"\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Handle the case when 'other' is a scalar (e.g., a float or int)\n",
    "        if isinstance(other, (int, float)) or isinstance(self, (int, float)):\n",
    "            # Scalar multiplication: Multiply the scalar with the data and return a new Tensor\n",
    "            out = Tensor(self.__data * other, requires_grad=self.__requires_grad)\n",
    "            out.parents = {self}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.__requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other  # Gradient w.r.t. the scalar\n",
    "                    else:\n",
    "                        self.grad += grad * other  # Accumulate gradient w.r.t. the scalar\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ScalarMulBackward\"\n",
    "            return out\n",
    "        \n",
    "        # Handle the case when 'other' is a Tensor\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.__data * other.data, requires_grad=self.__requires_grad or other.requires_grad)\n",
    "            out.parents = {self, other}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.__requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other.data  # Gradient w.r.t. the other Tensor\n",
    "                    else:\n",
    "                        self.grad += grad * other.data  # Accumulate gradient w.r.t. the other Tensor\n",
    "                if other.requires_grad:\n",
    "                    if other.grad is None:\n",
    "                        other.grad = grad * self.__data  # Gradient w.r.t. self Tensor\n",
    "                    else:\n",
    "                        other.grad += grad * self.__data  # Accumulate gradient w.r.t. self Tensor\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"TensorMulBackward\"\n",
    "            return out\n",
    "\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.__data / other.data, requires_grad=self.__requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / other.data\n",
    "                else:\n",
    "                    self.grad += grad / other.data\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad * self.__data / (other.data ** 2)\n",
    "                else:\n",
    "                    other.grad -= grad * self.__data / (other.data ** 2)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"DivBackward\"\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        out = Tensor(self.__data.mean(), requires_grad=self.__requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / self.__data.size\n",
    "                else:\n",
    "                    self.grad += grad / self.__data.size\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MeanBackward\"\n",
    "        return out\n",
    "\n",
    "    def sum(self):\n",
    "        out = Tensor(self.__data.sum(), requires_grad=self.__requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * np.ones_like(self.__data)\n",
    "                else:\n",
    "                    self.grad += grad * np.ones_like(self.__data)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"SumBackward\"\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # Apply ReLU: max(0, x)\n",
    "        out_data = np.maximum(self.__data, 0)\n",
    "\n",
    "        # Create a new tensor for the result\n",
    "        out = Tensor(out_data, requires_grad=self.__requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        if self.__requires_grad:\n",
    "            # Define the backward pass for ReLU\n",
    "            def _backward(grad):\n",
    "                # The derivative of ReLU is 1 for positive values, 0 for negative\n",
    "                relu_grad = (self.__data > 0).astype(float)  # Create mask for positive values\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * relu_grad\n",
    "                else:\n",
    "                    self.grad += grad * relu_grad\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ReLUBackward\"\n",
    "        return out\n",
    "\n",
    "    def softmax(self):\n",
    "        # Apply softmax to logits for numerical stability\n",
    "        max_logits = np.max(self.__data, axis=0, keepdims=True)  # Shape (1, N)\n",
    "        exps = np.exp(self.__data - max_logits)\n",
    "        sum_exps = np.sum(exps, axis=0, keepdims=True)\n",
    "        result = exps / sum_exps\n",
    "        # result = np.exp(self.__data) / sum(np.exp(self.__data))\n",
    "        \n",
    "        out = Tensor(result, requires_grad=self.__requires_grad)  # Output tensor\n",
    "        out.parents = {self}  # Store parent tensors\n",
    "\n",
    "        if self.__requires_grad:\n",
    "            def _backward(grad):\n",
    "                \n",
    "                # Compute softmax of the input\n",
    "                # softmax = exps / sum_exps  # Compute softmax\n",
    "                # Gradient of log-softmax\n",
    "                # grad_input = grad - np.sum(grad, axis=-1, keepdims=True) * softmax  # Backpropagate\n",
    "                grad_input = result * (grad - np.sum(grad * result, axis=0, keepdims=True))\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad_input  # Initialize grad if it's None\n",
    "                else:\n",
    "                    self.grad += grad_input  # Accumulate gradients if grad already exists\n",
    "\n",
    "                return grad  # Return gradient input for the next layer\n",
    "\n",
    "            out.grad_fn = _backward  # Store the backward function\n",
    "            out.grad_fn_name = \"LogSoftmaxBackward\"\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # def log(self):\n",
    "    #     # Handle log of zero by adding a small epsilon\n",
    "    #     out = Tensor(np.log(self.__data + 1e-9), requires_grad=self.__requires_grad)\n",
    "    #     out._prev = {self}\n",
    "\n",
    "    #     def _backward(grad):\n",
    "    #         if self.__requires_grad:\n",
    "    #             if self.grad is None:\n",
    "    #                 self.grad = grad / (self.__data + 1e-9)\n",
    "    #             else:\n",
    "    #                 self.grad += grad / (self.__data + 1e-9)\n",
    "\n",
    "    #     out.grad_fn = _backward\n",
    "    #     out.grad_fn_name = \"LogBackward\"\n",
    "    #     return out\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        out = Tensor(self.__data ** power, requires_grad=self.__requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * power * (self.__data ** (power - 1))\n",
    "                else:\n",
    "                    self.grad += grad * power * (self.__data ** (power - 1))\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"PowBackward\"\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.__data @ other.data, requires_grad=self.__requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.__requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad @ other.data.T\n",
    "                else:\n",
    "                    self.grad += grad @ other.data.T\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = self.__data.T @ grad\n",
    "                else:\n",
    "                    other.grad += self.__data.T @ grad\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MatMulBackward\"\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        grad_fn_str = f\", grad_fn=<{self.grad_fn_name}>\" if self.grad_fn else \"\"\n",
    "        return f\"Tensor({self.__data}, requires_grad={self.__requires_grad}{grad_fn_str})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \n",
    "        # Start the backward pass if this tensor requires gradients\n",
    "        if not self.__requires_grad:\n",
    "            raise ValueError(\"This tensor does not require gradients.\")\n",
    "        \n",
    "        # Initialize the gradient for the tensor if not already set\n",
    "        if self.grad is None:\n",
    "            self.grad = np.ones_like(self.__data)  # Start with gradient of 1 for scalar output\n",
    "            # self.grad = Tensor(self.grad)  # Convert to a tensor\n",
    "        \n",
    "        # A stack of tensors to backpropagate through\n",
    "        to_process = [self]\n",
    "        # Process the tensors in reverse order (topological order)\n",
    "        while to_process:\n",
    "            tensor = to_process.pop()\n",
    "            if tensor.is_leaf and tensor.data.shape != tensor.grad.shape:\n",
    "                tensor.grad = np.sum(tensor.grad,axis=1).reshape(-1,1)\n",
    "\n",
    "            # If this tensor has a backward function, call it\n",
    "            if tensor.grad_fn is not None:\n",
    "                # print(f\"Backpropagating through {tensor.grad_fn_name}\")\n",
    "                # Pass the gradient to the parent tensors\n",
    "                tensor.grad_fn(tensor.grad)\n",
    "                # print(tensor.grad)\n",
    "                # Add the parents of this tensor to the stack for backpropagation\n",
    "                to_process.extend(tensor.parents)\n",
    "                \n",
    "    def detach(self):\n",
    "        # Create a new tensor that shares the same data but has no gradient tracking\n",
    "        detached_tensor = Tensor(self.__data, requires_grad=False)\n",
    "        detached_tensor.grad = self.grad  # Retain the gradient (but no computation graph)\n",
    "        detached_tensor.parents = set()  # Detach from the computation graph\n",
    "        detached_tensor._grad_fn = None  # Remove the function responsible for backward\n",
    "        detached_tensor._grad_fn_name = None\n",
    "        return detached_tensor\n",
    "    \n",
    "    def T(self):\n",
    "        return Tensor(self.__data.T, requires_grad=self.__requires_grad, is_leaf=self.__is_leaf, dtype=self.__dtype)\n",
    "    \n",
    "def tensor(data, dtype=float32, requires_grad=False, is_leaf=False):\n",
    "    '''\n",
    "    Factory function, generates a tensor instance instead of calling the class,  \n",
    "    imitates the torch.tensor() function\n",
    "\n",
    "    ### parameters\n",
    "    - data: list or numeric  \n",
    "    - dtype: dtype, default float64  \n",
    "    - requires_grad: bool, default False  \n",
    "    - is_leaf: bool, default True  \n",
    "\n",
    "    ### returns\n",
    "    - Tensor instance\n",
    "\n",
    "    ```\n",
    "    >>> tensor([1,2,3])\n",
    "    Tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "    ```\n",
    "    \n",
    "    '''\n",
    "    return Tensor(data, dtype=dtype, requires_grad=requires_grad, is_leaf=is_leaf) \n",
    "\n",
    "def zeros(shape, dtype=float32, requires_grad=False, is_leaf=True):\n",
    "    '''\n",
    "    Factory function, generates a tensor of zeros instead of calling the class,  \n",
    "    imitates the torch.zeros() function\n",
    "\n",
    "    ### parameters\n",
    "    - shape: tuple  \n",
    "    - dtype: dtype, default float64  \n",
    "    - requires_grad: bool, default False  \n",
    "    - is_leaf: bool, default True  \n",
    "\n",
    "    ### returns\n",
    "    - Tensor instance\n",
    "\n",
    "    ```\n",
    "    >>> zeros((2,3))\n",
    "    Tensor([[0., 0., 0.],\n",
    "            [0., 0., 0.]], requires_grad=False)\n",
    "    ```\n",
    "    \n",
    "    '''\n",
    "    return Tensor(np.zeros(shape), dtype=dtype, requires_grad=requires_grad, is_leaf=is_leaf)\n",
    "\n",
    "def ones(shape, dtype=float32, requires_grad=False, is_leaf=True):\n",
    "    '''\n",
    "    Factory function, generates a tensor of ones instead of calling the class,  \n",
    "    imitates the torch.ones() function\n",
    "\n",
    "    ### parameters\n",
    "    - shape: tuple  \n",
    "    - dtype: dtype, default float64  \n",
    "    - requires_grad: bool, default False  \n",
    "    - is_leaf: bool, default True  \n",
    "\n",
    "    ### returns\n",
    "    - Tensor instance\n",
    "\n",
    "    ```\n",
    "    >>> ones((2,3))\n",
    "    Tensor([[1., 1., 1.],\n",
    "            [1., 1., 1.]], requires_grad=False)\n",
    "    ```\n",
    "    \n",
    "    '''\n",
    "    return Tensor(np.ones(shape), requires_grad, is_leaf, dtype=dtype)\n",
    "\n",
    "def randn(shape, dtype=float32, requires_grad=False, is_leaf=True):\n",
    "    '''\n",
    "    Factory function, generates a tensor of random numbers instead of calling the class,  \n",
    "    imitates the torch.randn() function\n",
    "\n",
    "    ### parameters\n",
    "    - shape: tuple  \n",
    "    - dtype: dtype, default float64  \n",
    "    - requires_grad: bool, default False  \n",
    "    - is_leaf: bool, default True  \n",
    "\n",
    "    ### returns\n",
    "    - Tensor instance\n",
    "\n",
    "    ```\n",
    "    >>> randn((2,3))\n",
    "    Tensor([[0.1, -0.2, 0.3],\n",
    "            [0.4, -0.5, 0.6]], requires_grad=False)\n",
    "    ```\n",
    "    \n",
    "    '''\n",
    "    return Tensor(np.random.randn(*shape), dtype=dtype, requires_grad=requires_grad, is_leaf=is_leaf)\n",
    "\n",
    "def tensor_like(tensor, data, requires_grad=False, is_leaf=False):\n",
    "    '''\n",
    "    Factory function, generates a tensor with the same shape as another tensor instead of calling the class,  \n",
    "    imitates the torch.tensor() function\n",
    "\n",
    "    ### parameters\n",
    "    - tensor: Tensor instance  \n",
    "    - data: list or numeric  \n",
    "    - requires_grad: bool, default False  \n",
    "    - is_leaf: bool, default True  \n",
    "\n",
    "    ### returns\n",
    "    - Tensor instance\n",
    "\n",
    "    ```\n",
    "    >>> a=tensor([1,2,3])\n",
    "    >>> tensor_like(a, [4,5,6])\n",
    "    Tensor([4.0, 5.0, 6.0], requires_grad=False)\n",
    "    ```\n",
    "    \n",
    "    '''\n",
    "    return Tensor(data, requires_grad, is_leaf, dtype=tensor.dtype)\n",
    " \n",
    "def transpose(tensor):\n",
    "    '''\n",
    "    generates a transposed tensor instead of calling the class method\n",
    "    '''\n",
    "    return tensor.T()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating input at att1\n",
      "attributes are: {'_Foo__att1': 1}\n",
      "checking self.__att1: 1\n"
     ]
    }
   ],
   "source": [
    "class Foo:\n",
    "    def __init__(self,att1):\n",
    "        self.__att1=att1\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_inp(att):\n",
    "        if att: #if not None, valid\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @property\n",
    "    def att1(self):\n",
    "        return self.__att1\n",
    "    @att1.setter\n",
    "    def att1(self,att1):\n",
    "        self.__att1=att1\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name == '_Foo__att1':\n",
    "            print(\"Validating input at att1\")\n",
    "            if Foo.validate_inp(value):\n",
    "                # self.__dict__['_Foo__att1'] = value #this works\n",
    "                super.__setattr__(self,'_Foo__att1',value)\n",
    "                print('attributes are:',self.__dict__)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid input\")\n",
    "        else:\n",
    "            self.__dict__[name] = value\n",
    "\n",
    "    def test(self):\n",
    "        print('checking self.__att1:',self.__att1)\n",
    "\n",
    "f=Foo(1)\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: Tensor(1.0,float64)\n",
      "dimensions:[],ndim:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../modules')\n",
    "from tensor import *\n",
    "\n",
    "scalar=tensor(1)\n",
    "print('scalar:',scalar)\n",
    "print(f'dimensions:{scalar.shape},ndim:{scalar.ndim}')\n",
    "\n",
    "vector=tensor([1,2,3])\n",
    "# print('vector:',vector)\n",
    "# print(f'dimensions:{vector.shape},ndim:{vector.ndim}')\n",
    "matrix=tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 36, 42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_multiply([1,2,3],[[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: requires_grad.setter, requires_grad must be a boolean\n"
     ]
    }
   ],
   "source": [
    "matrix.requires_grad=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.dtype='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[1, 2, 3], [4, 5, 6]], dtype=int64, requires_grad=False, is_leaf=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix.dtype='int32'\n",
    "matrix.dtype='int64'\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 10, 12], [14, 16, 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_nested_lists(list1, list2):\n",
    "    '''this performs addition on multidimensional lists (including numerics)'''\n",
    "    if isinstance(list1, list) and isinstance(list2, list):\n",
    "        return [add_nested_lists(x, y) for x, y in zip(list1, list2)]\n",
    "    else:\n",
    "        return list1 + list2\n",
    "\n",
    "list1 = [[1, 2, 3], [4, 5, 6]]\n",
    "list2 = [[7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "result = add_nested_lists(list1, list2)\n",
    "print(result)\n",
    "\n",
    "add_nested_lists(1,1) #works well for scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t=torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]], [[19,20,21],[22,23,24]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]],\n",
       "\n",
       "        [[19, 20, 21],\n",
       "         [22, 23, 24]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  7, 13, 19],\n",
       "         [ 4, 10, 16, 22]],\n",
       "\n",
       "        [[ 2,  8, 14, 20],\n",
       "         [ 5, 11, 17, 23]],\n",
       "\n",
       "        [[ 3,  9, 15, 21],\n",
       "         [ 6, 12, 18, 24]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4, 7), (2, 5, 8), (3, 6, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transpose_2dlist(list1):\n",
    "    '''this performs transpose on multidimensional lists'''\n",
    "    if isinstance(list1, list):\n",
    "        return [transpose_2dlist(x) for x in zip(*list1)]\n",
    "    else:\n",
    "        return list1\n",
    "    \n",
    "list1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "list2=[[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]], [[19,20,21],[22,23,24]]]\n",
    "transpose_2dlist(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: len(), tensor is a scalar with 0 dimensions\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m transpose_2dlist(list1)\n\u001b[1;32m     11\u001b[0m     transpose_recursive(matrix, ndim)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtranspose_3dlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36mtranspose_3dlist\u001b[0;34m(list1)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transpose_2dlist(list1)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtranspose_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36mtranspose_recursive\u001b[0;34m(lst, depth)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lst\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtranspose_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lst[\u001b[38;5;241m0\u001b[39m]))]\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36mtranspose_recursive\u001b[0;34m(lst, depth)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lst\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [transpose_recursive([row[i] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m lst], depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def transpose_recursive(lst, depth):\n",
    "    if depth == 1:\n",
    "        return lst\n",
    "    return [transpose_recursive([row[i] for row in lst], depth - 1) for i in range(len(lst[0]))]\n",
    "\n",
    "def transpose_3dlist(list1):\n",
    "    '''this performs transpose on multidimensional lists'''\n",
    "    ndim=tensor(list1).ndim\n",
    "    if ndim==2:\n",
    "        return transpose_2dlist(list1)\n",
    "    transpose_recursive(matrix, ndim)\n",
    "\n",
    "transpose_3dlist(list2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
