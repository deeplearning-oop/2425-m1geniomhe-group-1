{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, is_leaf=False):    \n",
    "        self.data = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "        self.requires_grad = requires_grad\n",
    "        self.is_leaf = is_leaf\n",
    "        self.grad = None\n",
    "        self.grad_fn = None\n",
    "        self.grad_fn_name = None\n",
    "        self.parents = set()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''important for dataset and dataloaders'''\n",
    "        return self.data[idx]\n",
    "\n",
    "    # ----- to flatten images --------\n",
    "    def view(self,*args):\n",
    "        '''same as torch's functionality, it collapses all dimensions into 1\n",
    "        \n",
    "        <!> to be tested sperately on example tensors\n",
    "        '''\n",
    "        nd_array=self.data\n",
    "        reshaped= nd_array.reshape(args)\n",
    "        t= self\n",
    "        t.data=reshaped\n",
    "        return t\n",
    "    \n",
    "    def flatten_batch(self):\n",
    "        '''\n",
    "        given that a tenosr is a batch of length batch_size, it'll flatten the dimensions while conserving the batch dimension (and transpsoing to match pytorch's behavior)\n",
    "    \n",
    "        e.g., it will take (batch_size,1,28,28) and return (784, batch_size) \n",
    "\n",
    "        <!> used for testing while batch training images\n",
    "        '''\n",
    "        flattened = np.array([img.flatten() for img in self.data])  # Shape: (32, 784)\n",
    "        transposed = flattened.T  # Shape: (784, 32)\n",
    "\n",
    "        self.data = transposed\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.data + other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"AddBackward\"\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        \n",
    "        result = Tensor(-self.data, requires_grad=self.requires_grad)\n",
    "        result.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = -grad\n",
    "                else:\n",
    "                    self.grad -= grad\n",
    "\n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"NegBackward\"\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.data - other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        result.parents = {self, other}\n",
    "        \n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad\n",
    "                else:\n",
    "                    other.grad -= grad\n",
    "        \n",
    "        result.grad_fn = _backward\n",
    "        result.grad_fn_name = \"SubBackward\"\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Handle the case when 'other' is a scalar (e.g., a float or int)\n",
    "        if isinstance(other, (int, float)) or isinstance(self, (int, float)):\n",
    "            # Scalar multiplication: Multiply the scalar with the data and return a new Tensor\n",
    "            out = Tensor(self.data * other, requires_grad=self.requires_grad)\n",
    "            out.parents = {self}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other  # Gradient w.r.t. the scalar\n",
    "                    else:\n",
    "                        self.grad += grad * other  # Accumulate gradient w.r.t. the scalar\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ScalarMulBackward\"\n",
    "            return out\n",
    "        \n",
    "        # Handle the case when 'other' is a Tensor\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.data * other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "            out.parents = {self, other}\n",
    "\n",
    "            def _backward(grad):\n",
    "                if self.requires_grad:\n",
    "                    if self.grad is None:\n",
    "                        self.grad = grad * other.data  # Gradient w.r.t. the other Tensor\n",
    "                    else:\n",
    "                        self.grad += grad * other.data  # Accumulate gradient w.r.t. the other Tensor\n",
    "                if other.requires_grad:\n",
    "                    if other.grad is None:\n",
    "                        other.grad = grad * self.data  # Gradient w.r.t. self Tensor\n",
    "                    else:\n",
    "                        other.grad += grad * self.data  # Accumulate gradient w.r.t. self Tensor\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"TensorMulBackward\"\n",
    "            return out\n",
    "\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data / other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / other.data\n",
    "                else:\n",
    "                    self.grad += grad / other.data\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = -grad * self.data / (other.data ** 2)\n",
    "                else:\n",
    "                    other.grad -= grad * self.data / (other.data ** 2)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"DivBackward\"\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        out = Tensor(self.data.mean(), requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad / self.data.size\n",
    "                else:\n",
    "                    self.grad += grad / self.data.size\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MeanBackward\"\n",
    "        return out\n",
    "\n",
    "    def sum(self):\n",
    "        out = Tensor(self.data.sum(), requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * np.ones_like(self.data)\n",
    "                else:\n",
    "                    self.grad += grad * np.ones_like(self.data)\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"SumBackward\"\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        # Apply ReLU: max(0, x)\n",
    "        out_data = np.maximum(self.data, 0)\n",
    "\n",
    "        # Create a new tensor for the result\n",
    "        out = Tensor(out_data, requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "        if self.requires_grad:\n",
    "            # Define the backward pass for ReLU\n",
    "            def _backward(grad):\n",
    "                # The derivative of ReLU is 1 for positive values, 0 for negative\n",
    "                relu_grad = (self.data > 0).astype(float)  # Create mask for positive values\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * relu_grad\n",
    "                else:\n",
    "                    self.grad += grad * relu_grad\n",
    "\n",
    "            out.grad_fn = _backward\n",
    "            out.grad_fn_name = \"ReLUBackward\"\n",
    "        return out\n",
    "\n",
    "    def softmax(self):\n",
    "        # Apply softmax to logits for numerical stability\n",
    "        max_logits = np.max(self.data, axis=0, keepdims=True)  # Shape (1, N)\n",
    "        exps = np.exp(self.data - max_logits)\n",
    "        sum_exps = np.sum(exps, axis=0, keepdims=True)\n",
    "        result = exps / sum_exps\n",
    "        # result = np.exp(self.data) / sum(np.exp(self.data))\n",
    "        \n",
    "        out = Tensor(result, requires_grad=self.requires_grad)  # Output tensor\n",
    "        out.parents = {self}  # Store parent tensors\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _backward(grad):\n",
    "                \n",
    "                # Compute softmax of the input\n",
    "                # softmax = exps / sum_exps  # Compute softmax\n",
    "                # Gradient of log-softmax\n",
    "                # grad_input = grad - np.sum(grad, axis=-1, keepdims=True) * softmax  # Backpropagate\n",
    "                grad_input = result * (grad - np.sum(grad * result, axis=0, keepdims=True))\n",
    "\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad_input  # Initialize grad if it's None\n",
    "                else:\n",
    "                    self.grad += grad_input  # Accumulate gradients if grad already exists\n",
    "\n",
    "                return grad  # Return gradient input for the next layer\n",
    "\n",
    "            out.grad_fn = _backward  # Store the backward function\n",
    "            out.grad_fn_name = \"LogSoftmaxBackward\"\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # def log(self):\n",
    "    #     # Handle log of zero by adding a small epsilon\n",
    "    #     out = Tensor(np.log(self.data + 1e-9), requires_grad=self.requires_grad)\n",
    "    #     out._prev = {self}\n",
    "\n",
    "    #     def _backward(grad):\n",
    "    #         if self.requires_grad:\n",
    "    #             if self.grad is None:\n",
    "    #                 self.grad = grad / (self.data + 1e-9)\n",
    "    #             else:\n",
    "    #                 self.grad += grad / (self.data + 1e-9)\n",
    "\n",
    "    #     out.grad_fn = _backward\n",
    "    #     out.grad_fn_name = \"LogBackward\"\n",
    "    #     return out\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        out = Tensor(self.data ** power, requires_grad=self.requires_grad)\n",
    "        out.parents = {self}\n",
    "\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad * power * (self.data ** (power - 1))\n",
    "                else:\n",
    "                    self.grad += grad * power * (self.data ** (power - 1))\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"PowBackward\"\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data @ other.data, requires_grad=self.requires_grad or other.requires_grad)\n",
    "        out.parents = {self, other}\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad @ other.data.T\n",
    "                else:\n",
    "                    self.grad += grad @ other.data.T\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = self.data.T @ grad\n",
    "                else:\n",
    "                    other.grad += self.data.T @ grad\n",
    "\n",
    "        out.grad_fn = _backward\n",
    "        out.grad_fn_name = \"MatMulBackward\"\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        grad_fn_str = f\", grad_fn=<{self.grad_fn_name}>\" if self.grad_fn else \"\"\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad}{grad_fn_str})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \n",
    "        # Start the backward pass if this tensor requires gradients\n",
    "        if not self.requires_grad:\n",
    "            raise ValueError(\"This tensor does not require gradients.\")\n",
    "        \n",
    "        # Initialize the gradient for the tensor if not already set\n",
    "        if self.grad is None:\n",
    "            self.grad = np.ones_like(self.data)  # Start with gradient of 1 for scalar output\n",
    "            # self.grad = Tensor(self.grad)  # Convert to a tensor\n",
    "        \n",
    "        # A stack of tensors to backpropagate through\n",
    "        to_process = [self]\n",
    "        # Process the tensors in reverse order (topological order)\n",
    "        while to_process:\n",
    "            tensor = to_process.pop()\n",
    "            if tensor.is_leaf and tensor.data.shape != tensor.grad.shape:\n",
    "                tensor.grad = np.sum(tensor.grad,axis=1).reshape(-1,1)\n",
    "\n",
    "            # If this tensor has a backward function, call it\n",
    "            if tensor.grad_fn is not None:\n",
    "                # print(f\"Backpropagating through {tensor.grad_fn_name}\")\n",
    "                # Pass the gradient to the parent tensors\n",
    "                tensor.grad_fn(tensor.grad)\n",
    "                # print(tensor.grad)\n",
    "                # Add the parents of this tensor to the stack for backpropagation\n",
    "                to_process.extend(tensor.parents)\n",
    "                \n",
    "    def detach(self):\n",
    "        # Create a new tensor that shares the same data but has no gradient tracking\n",
    "        detached_tensor = Tensor(self.data, requires_grad=False)\n",
    "        detached_tensor.grad = self.grad  # Retain the gradient (but no computation graph)\n",
    "        detached_tensor.parents = set()  # Detach from the computation graph\n",
    "        detached_tensor._grad_fn = None  # Remove the function responsible for backward\n",
    "        detached_tensor._grad_fn_name = None\n",
    "        return detached_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    '''----------------- Data Types -----------------\n",
    "    ---------------------------------------------------\n",
    "    this Enum class is used to define the data types of the tensors,\n",
    "    it allows restricting the data types to the ones defined in the class\n",
    "    as well as easily converting the data types to numpy data types\n",
    "\n",
    "    Mechanism of action relies on:  \n",
    "        * access of dtype calue from a string representation of the data type\n",
    "        * __call__ method that allows conversion of data to the specified data type through numpy\n",
    "    \n",
    "        p.s. uint8 is used for images as pixel values are in the range [0,255] which fits exactly to 8 bits (2^8=256)  \n",
    "            => uint8 is the first unit of conversion from image to tensor and a memory saver\n",
    "            \n",
    "    ____________________________________________________________________________________________________________________________\n",
    "\n",
    "    '''\n",
    "\n",
    "    int32='int32'\n",
    "    int64='int64'\n",
    "    float32='float32'\n",
    "    float64='float64'\n",
    "    uint8='uint8'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        >>> DataType.float32([1,2,3])\n",
    "        array([1., 2., 3.], dtype=float32)\n",
    "        >>> dtype=DataType.int32  \n",
    "        >>> dtype(1.7)\n",
    "        array(1, dtype=int32)\n",
    "        '''\n",
    "        return np.array(x, dtype=self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dtype=DataType('uint8')\n",
    "a=dtype(1.7)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating input at att1\n",
      "attributes are: {'_Foo__att1': 1}\n",
      "checking self.__att1: 1\n"
     ]
    }
   ],
   "source": [
    "class Foo:\n",
    "    def __init__(self,att1):\n",
    "        self.__att1=att1\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_inp(att):\n",
    "        if att: #if not None, valid\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @property\n",
    "    def att1(self):\n",
    "        return self.__att1\n",
    "    @att1.setter\n",
    "    def att1(self,att1):\n",
    "        self.__att1=att1\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name == '_Foo__att1':\n",
    "            print(\"Validating input at att1\")\n",
    "            if Foo.validate_inp(value):\n",
    "                # self.__dict__['_Foo__att1'] = value #this works\n",
    "                super.__setattr__(self,'_Foo__att1',value)\n",
    "                print('attributes are:',self.__dict__)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid input\")\n",
    "        else:\n",
    "            self.__dict__[name] = value\n",
    "\n",
    "    def test(self):\n",
    "        print('checking self.__att1:',self.__att1)\n",
    "\n",
    "f=Foo(1)\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: Tensor(1.0,float64)\n",
      "dimensions:[],ndim:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../modules')\n",
    "from tensor import *\n",
    "\n",
    "scalar=tensor(1)\n",
    "print('scalar:',scalar)\n",
    "print(f'dimensions:{scalar.shape},ndim:{scalar.ndim}')\n",
    "\n",
    "vector=tensor([1,2,3])\n",
    "# print('vector:',vector)\n",
    "# print(f'dimensions:{vector.shape},ndim:{vector.ndim}')\n",
    "matrix=tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 36, 42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_multiply([1,2,3],[[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: requires_grad.setter, requires_grad must be a boolean\n"
     ]
    }
   ],
   "source": [
    "matrix.requires_grad=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.dtype='float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[1, 2, 3], [4, 5, 6]], dtype=int64, requires_grad=False, is_leaf=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix.dtype='int32'\n",
    "matrix.dtype='int64'\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 10, 12], [14, 16, 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_nested_lists(list1, list2):\n",
    "    '''this performs addition on multidimensional lists (including numerics)'''\n",
    "    if isinstance(list1, list) and isinstance(list2, list):\n",
    "        return [add_nested_lists(x, y) for x, y in zip(list1, list2)]\n",
    "    else:\n",
    "        return list1 + list2\n",
    "\n",
    "list1 = [[1, 2, 3], [4, 5, 6]]\n",
    "list2 = [[7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "result = add_nested_lists(list1, list2)\n",
    "print(result)\n",
    "\n",
    "add_nested_lists(1,1) #works well for scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t=torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]], [[19,20,21],[22,23,24]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]],\n",
       "\n",
       "        [[19, 20, 21],\n",
       "         [22, 23, 24]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  7, 13, 19],\n",
       "         [ 4, 10, 16, 22]],\n",
       "\n",
       "        [[ 2,  8, 14, 20],\n",
       "         [ 5, 11, 17, 23]],\n",
       "\n",
       "        [[ 3,  9, 15, 21],\n",
       "         [ 6, 12, 18, 24]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4, 7), (2, 5, 8), (3, 6, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transpose_2dlist(list1):\n",
    "    '''this performs transpose on multidimensional lists'''\n",
    "    if isinstance(list1, list):\n",
    "        return [transpose_2dlist(x) for x in zip(*list1)]\n",
    "    else:\n",
    "        return list1\n",
    "    \n",
    "list1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "list2=[[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]], [[19,20,21],[22,23,24]]]\n",
    "transpose_2dlist(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: len(), tensor is a scalar with 0 dimensions\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m transpose_2dlist(list1)\n\u001b[1;32m     11\u001b[0m     transpose_recursive(matrix, ndim)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtranspose_3dlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36mtranspose_3dlist\u001b[0;34m(list1)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transpose_2dlist(list1)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtranspose_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36mtranspose_recursive\u001b[0;34m(lst, depth)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lst\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtranspose_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lst[\u001b[38;5;241m0\u001b[39m]))]\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36mtranspose_recursive\u001b[0;34m(lst, depth)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lst\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [transpose_recursive([row[i] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m lst], depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def transpose_recursive(lst, depth):\n",
    "    if depth == 1:\n",
    "        return lst\n",
    "    return [transpose_recursive([row[i] for row in lst], depth - 1) for i in range(len(lst[0]))]\n",
    "\n",
    "def transpose_3dlist(list1):\n",
    "    '''this performs transpose on multidimensional lists'''\n",
    "    ndim=tensor(list1).ndim\n",
    "    if ndim==2:\n",
    "        return transpose_2dlist(list1)\n",
    "    transpose_recursive(matrix, ndim)\n",
    "\n",
    "transpose_3dlist(list2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
