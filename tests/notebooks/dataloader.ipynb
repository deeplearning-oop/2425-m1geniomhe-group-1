{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing dataloader like torch:  \n",
    "\n",
    "\n",
    "- it's iterable  \n",
    "- not indexable however  \n",
    "- no string representation  \n",
    "- it takes every batch of data in a tensor form: if we have 60000 `(1,28,28)` tensors, it's return a loader with 938 tensors where each is of dimensions `(64,1,28,28)`(last one will be `(32,1,28,28)` if we have 60000 samples). It compresses every 64 (batch_size) tensors into one tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='mnist_data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='mnist_data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60032"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "938*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_loader[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    \n",
    "    if i[0].shape[0] != 64:\n",
    "        print(i[0].shape)\n",
    "        print(i[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: mnist_data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'num_workers', 'prefetch_factor', 'pin_memory', 'pin_memory_device', 'timeout', 'worker_init_fn', '_DataLoader__multiprocessing_context', '_dataset_kind', 'batch_size', 'drop_last', 'sampler', 'batch_sampler', 'generator', 'collate_fn', 'persistent_workers', '_DataLoader__initialized', '_IterableDataset_len_called', '_iterator'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__.keys()\n",
    "# train_loader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      4\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdataloader\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "\n",
    "from ..modules.tensor import *\n",
    "from ..modules.dataset import *\n",
    "\n",
    "class dataloader:\n",
    "    '''DataLoader class\n",
    "    --------------------------------\n",
    "    The purpose is to make an object out of the dataset that is split into batches and is iterable to be used in the training loop (even in the validation loop).\n",
    "    \n",
    "    Implementing dataloader like torch:  \n",
    "\n",
    "    - it's iterable  \n",
    "    - not indexable however  \n",
    "    - no string representation  \n",
    "    - it takes every batch of data in a tensor form: if we have 60000 `(1,28,28)` tensors, it's return a loader with 938 tensors where each is of dimensions `(64,1,28,28)`(last one will be `(32,1,28,28)` if we have 60000 samples). It compresses every 64 (batch_size) tensors into one tensor.  \n",
    "    \n",
    "    <!> next step would be to make it do parallel processing (multiprocessing) to speed up the process (only start with it when everything works right)\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.__dataset=dataset  \n",
    "        self.__batch_size=batch_size  \n",
    "        self.__shuffle = shuffle\n",
    "        self.__num_samples = len(dataset)  \n",
    "\n",
    "\n",
    "    # -- getters and setters --\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.__dataset\n",
    "    @dataset.setter\n",
    "    def dataset(self, dataset):\n",
    "        self.__dataset = dataset\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__batch_size\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, batch_size):\n",
    "        self.__batch_size = batch_size\n",
    "    \n",
    "    @property\n",
    "    def shuffle(self):\n",
    "        return self.__shuffle\n",
    "    @shuffle.setter\n",
    "    def shuffle(self, shuffle):\n",
    "        self.__shuffle = shuffle\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return self.__num_samples\n",
    "    @num_samples.setter\n",
    "    def num_samples(self, num_samples):\n",
    "        self.__num_samples = num_samples\n",
    "\n",
    "    # -- need to validate the setters --\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        '''\n",
    "        validating the attribute types  (dataset must be a torch.utils.data.Dataset object, batch_size must be an integer, shuffle must be a boolean fo instance\n",
    "\n",
    "        Handle the errors and maybe set to default parameters.        \n",
    "        '''\n",
    "\n",
    "        if name == 'dataset':\n",
    "            # !!!!!!!! dont forget its abstract class !!!!!!!!!\n",
    "            # if not isinstance(value, Dataset):\n",
    "            #     raise TypeError('dataset must be a torch.utils.data.Dataset object')\n",
    "            pass\n",
    "        elif name == 'batch_size':\n",
    "            if not isinstance(value, int):\n",
    "                raise TypeError('batch_size must be an integer')\n",
    "        elif name == 'shuffle':\n",
    "            if not isinstance(value, bool):\n",
    "                raise TypeError('shuffle must be a boolean')\n",
    "        super().__setattr__(name, value)\n",
    "        \n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''iterability\n",
    "        \n",
    "        each batch_size tensors that come in the dataset will be compressed in one tensor, adding one dimension at the beginning\n",
    "\n",
    "        example:   \n",
    "            * batch_size=64  \n",
    "            * dataset is a `(60000, 28, 28)` tensor (each item is `(1,28,28)`)  \n",
    "            * Each item of the dataloader will be a `(64, 1, 28, 28)` tensor consisting of 64 items of the dataset tensor\n",
    "        '''\n",
    "        shuffled_indices = list(range(self.num_samples))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(shuffled_indices)\n",
    "\n",
    "        for i in range(0, self.num_samples, self.batch_size):\n",
    "            indices = shuffled_indices[i:i+self.batch_size]\n",
    "\n",
    "            tensor_data_list=[self.dataset.data[i] for i in indices]\n",
    "            nd_data_list=[tensor_data_list[i].data for i in indices]\n",
    "\n",
    "            nd_data = np.stack(nd_data_list, axis=0)\n",
    "            tensor_data=Tensor(nd_data)  \n",
    "\n",
    "            print('<> tetsing the dataloader')\n",
    "            print('     indices:', indices)\n",
    "            print('     tensor_data_list:', tensor_data_list)\n",
    "            print('     tensor_data list item shape :', tensor_data_list[0].shape)\n",
    "            print('     tensor_data.shape:', tensor_data.shape)  \n",
    "            print('     nd_data.shape:', nd_data.shape) \n",
    "\n",
    "            yield tensor_data\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 11],\n",
       "       [ 2, 12],\n",
       "       [ 3, 13],\n",
       "       [ 4, 14],\n",
       "       [ 5, 15],\n",
       "       [ 6, 16],\n",
       "       [ 7, 17],\n",
       "       [ 8, 18],\n",
       "       [ 9, 19],\n",
       "       [10, 20]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a1=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "a2=np.array([11,12,13,14,15,16,17,18,19,20])\n",
    "a3=np.array([21,22,23,24,25,26,27,28,29,30])\n",
    "a4=np.array([31,32,33,34,35,36,37,38,39,40])\n",
    "\n",
    "# collate each 2 arrays into one\n",
    "np.stack([a1,a2], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
